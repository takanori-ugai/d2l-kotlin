{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c267169",
   "metadata": {},
   "source": [
    "# Transformers for Vision\n",
    ":label:`sec_vision-transformer`\n",
    "\n",
    "The Transformer architecture was initially proposed \n",
    "for sequence to sequence learning, \n",
    "with a focus on machine translation. \n",
    "Subsequently, Transformers emerged as the model of choice \n",
    "in various natural language processing tasks :cite:`Radford.Narasimhan.Salimans.ea.2018,Radford.Wu.Child.ea.2019,brown2020language,Devlin.Chang.Lee.ea.2018,raffel2020exploring`. \n",
    "However, in the field of computer vision\n",
    "the dominant architecture has remained\n",
    "the CNN (:numref:`chap_modern_cnn`).\n",
    "Naturally, researchers started to wonder\n",
    "if it might be possible to do better\n",
    "by adapting Transformer models to image data.\n",
    "This question sparked immense interest\n",
    "in the computer vision community.\n",
    "Recently, :citet:`ramachandran2019stand` proposed \n",
    "a scheme for replacing convolution with self-attention. \n",
    "However, its use of specialized patterns in attention \n",
    "makes it hard to scale up models on hardware accelerators.\n",
    "Then, :citet:`cordonnier2020relationship` theoretically proved \n",
    "that self-attention can learn to behave similarly to convolution. \n",
    "Empirically, $2 \\times 2$ patches were taken from images as inputs, \n",
    "but the small patch size makes the model \n",
    "only applicable to image data with low resolutions.\n",
    "\n",
    "Without specific constraints on patch size,\n",
    "*vision Transformers* (ViTs)\n",
    "extract patches from images\n",
    "and feed them into a Transformer encoder\n",
    "to obtain a global representation,\n",
    "which will finally be transformed for classification :cite:`Dosovitskiy.Beyer.Kolesnikov.ea.2021`.\n",
    "Notably, Transformers show better scalability than CNNs:\n",
    "when training larger models on larger datasets,\n",
    "vision Transformers outperform ResNets by a significant margin. \n",
    "Similar to the landscape of network architecture design in natural language processing,\n",
    "Transformers also became a game-changer in computer vision.\n",
    "\n",
    "\n",
    "## Model\n",
    "\n",
    ":numref:`fig_vit` depicts\n",
    "the model architecture of vision Transformers.\n",
    "This architecture consists of a stem\n",
    "that patchifies images, \n",
    "a body based on the multi-layer Transformer encoder,\n",
    "and a head that transforms the global representation\n",
    "into the output label.\n",
    "\n",
    "![The vision Transformer architecture. In this example, an image is split into 9 patches. A special \u201c&lt;cls&gt;\u201d token and the 9 flattened image patches are transformed via patch embedding and $n$ Transformer encoder blocks into 10 representations, respectively. The \u201c&lt;cls&gt;\u201d representation is further transformed into the output label.](../img/vit.svg)\n",
    ":label:`fig_vit`\n",
    "\n",
    "Consider an input image with height $h$, width $w$,\n",
    "and $c$ channels.\n",
    "Specifying the patch height and width both as $p$,\n",
    "the image is split into a sequence of $m = hw/p^2$ patches,\n",
    "where each patch is flattened to a vector of length $cp^2$.\n",
    "In this way, image patches can be treated similarly to tokens in text sequences by Transformer encoders.\n",
    "A special \u201c&lt;cls&gt;\u201d (class) token and\n",
    "the $m$ flattened image patches are linearly projected\n",
    "into a sequence of $m+1$ vectors,\n",
    "summed with learnable positional embeddings.\n",
    "The multi-layer Transformer encoder\n",
    "transforms $m+1$ input vectors\n",
    "into the same amount of output vector representations of the same length.\n",
    "It works exactly the same way as the original Transformer encoder in :numref:`fig_transformer`,\n",
    "only differing in the position of normalization.\n",
    "Since the \u201c&lt;cls&gt;\u201d token attends to all the image patches \n",
    "via self-attention (see :numref:`fig_cnn-rnn-self-attention`),\n",
    "its representation from the Transformer encoder output\n",
    "will be further transformed into the output label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc20f00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"KwV8gD\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "       if(!window.letsPlotCallQueue) {\n",
       "           window.letsPlotCallQueue = [];\n",
       "       }; \n",
       "       window.letsPlotCall = function(f) {\n",
       "           window.letsPlotCallQueue.push(f);\n",
       "       };\n",
       "       (function() {\n",
       "           var script = document.createElement(\"script\");\n",
       "           script.type = \"text/javascript\";\n",
       "           script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.4.0/js-package/distr/lets-plot.min.js\";\n",
       "           script.onload = function() {\n",
       "               window.letsPlotCall = function(f) {f();};\n",
       "               window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "               window.letsPlotCallQueue = [];\n",
       "               \n",
       "               \n",
       "           };\n",
       "           script.onerror = function(event) {\n",
       "               window.letsPlotCall = function(f) {};\n",
       "               window.letsPlotCallQueue = [];\n",
       "               var div = document.createElement(\"div\");\n",
       "               div.style.color = 'darkred';\n",
       "               div.textContent = 'Error loading Lets-Plot JS';\n",
       "               document.getElementById(\"KwV8gD\").appendChild(div);\n",
       "           };\n",
       "           var e = document.getElementById(\"KwV8gD\");\n",
       "           e.appendChild(script);\n",
       "       })();\n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%use @file[../djl.json]\n",
    "%use lets-plot\n",
    "@file:DependsOn(\"../D2J-1.0-SNAPSHOT.jar\")\n",
    "import ai.djl.modality.nlp.DefaultVocabulary\n",
    "import ai.djl.modality.nlp.Vocabulary\n",
    "import ai.djl.modality.nlp.embedding.TrainableWordEmbedding\n",
    "import jp.live.ugai.d2j.timemachine.RNNModelScratch\n",
    "import jp.live.ugai.d2j.timemachine.TimeMachine.trainCh8\n",
    "import jp.live.ugai.d2j.timemachine.TimeMachineDataset\n",
    "import jp.live.ugai.d2j.timemachine.Vocab\n",
    "import jp.live.ugai.d2j.RNNModel\n",
    "import jp.live.ugai.d2j.util.StopWatch\n",
    "import jp.live.ugai.d2j.util.Accumulator\n",
    "import jp.live.ugai.d2j.util.Training\n",
    "import jp.live.ugai.d2j.util.TrainingChapter9\n",
    "import jp.live.ugai.d2j.lstm.Decoder\n",
    "import jp.live.ugai.d2j.lstm.Encoder\n",
    "import jp.live.ugai.d2j.lstm.EncoderDecoder\n",
    "import jp.live.ugai.d2j.util.NMT\n",
    "import jp.live.ugai.d2j.attention.AttentionDecoder\n",
    "import jp.live.ugai.d2j.attention.MultiHeadAttention\n",
    "import jp.live.ugai.d2j.PositionalEncoding\n",
    "import jp.live.ugai.d2j.MaskedSoftmaxCELoss\n",
    "import java.util.Locale\n",
    "import kotlin.random.Random\n",
    "import kotlin.collections.List\n",
    "import kotlin.collections.Map\n",
    "import kotlin.Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9660f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ai.djl.basicdataset.cv.classification.FashionMnist\n",
    "import ai.djl.metric.Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31b5677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    val manager = NDManager.newBaseManager()\n",
    "    val ps = ParameterStore(manager, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ba1da",
   "metadata": {},
   "source": [
    "## Patch Embedding\n",
    "\n",
    "To implement a vision Transformer, let's start \n",
    "with patch embedding in :numref:`fig_vit`. \n",
    "Splitting an image into patches \n",
    "and linearly projecting these flattened patches\n",
    "can be simplified as a single convolution operation, \n",
    "where both the kernel size and the stride size are set to the patch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d87547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(imgSize: Int = 96, val patchSize: Int = 16, val numHiddens: Int = 512) : AbstractBlock() {\n",
    "    val numPatches = (imgSize / patchSize) * (imgSize / patchSize)\n",
    "    val conv = Conv2d.builder()\n",
    "        .setKernelShape(Shape(patchSize.toLong(), patchSize.toLong()))\n",
    "        .optStride(Shape(patchSize.toLong(), patchSize.toLong()))\n",
    "        .setFilters(numHiddens)\n",
    "        .build()\n",
    "\n",
    "    override fun forwardInternal(\n",
    "        parameterStore: ParameterStore,\n",
    "        inputs: NDList,\n",
    "        training: Boolean,\n",
    "        params: PairList<String, Any>?\n",
    "    ): NDList {\n",
    "        // Output shape: (batch size, no. of patches, no. of channels)\n",
    "        val f = conv.forward(parameterStore, inputs, training, params).head()\n",
    "        return NDList(f.reshape(Shape(f.shape[0], f.shape[1], -1)).transpose(0, 2, 1))\n",
    "    }\n",
    "\n",
    "    override fun initializeChildBlocks(manager: NDManager, dataType: DataType, vararg inputShapes: Shape) {\n",
    "        conv.initialize(manager, dataType, *inputShapes)\n",
    "    }\n",
    "\n",
    "    /* We won't implement this since we won't be using it but it's required as part of an AbstractBlock  */\n",
    "    override fun getOutputShapes(inputShapes: Array<Shape>): Array<Shape> {\n",
    "        return arrayOf(Shape(inputShapes[0][0], numPatches.toLong(), numHiddens.toLong()))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dee27cf",
   "metadata": {},
   "source": [
    "In the following example, taking images with height and width of `img_size` as inputs,\n",
    "the patch embedding outputs `(img_size//patch_size)**2` patches \n",
    "that are linearly projected to vectors of length `num_hiddens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20c5b063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDList size: 1\n",
      "0 : (4, 36, 512) float32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    val manager = NDManager.newBaseManager()\n",
    "    val ps = ParameterStore(manager, false)\n",
    "    val imgSize = 96\n",
    "    val patchSize = 16\n",
    "    val numHiddens = 512\n",
    "    val batchSize = 4\n",
    "    val patchEmb = PatchEmbedding(imgSize, patchSize, numHiddens)\n",
    "    val X = manager.randomNormal(Shape(batchSize.toLong(), 3, imgSize.toLong(), imgSize.toLong()))\n",
    "    patchEmb.initialize(manager, DataType.FLOAT32, X.shape)\n",
    "    println(patchEmb.forward(ps, NDList(X), false))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab682a33",
   "metadata": {},
   "source": [
    "## Vision Transformer Encoder\n",
    ":label:`subsec_vit-encoder`\n",
    "\n",
    "The MLP of the vision Transformer encoder is slightly different \n",
    "from the position-wise FFN of the original Transformer encoder \n",
    "(see :numref:`subsec_positionwise-ffn`).\n",
    "First, here the activation function uses the Gaussian error linear unit (GELU),\n",
    "which can be considered as a smoother version of the ReLU :cite:`hendrycks2016gaussian`.\n",
    "Second, dropout is applied to the output of each fully connected layer in the MLP for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3859504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTMLP(mlpNumHiddens: Int, mlpNumOutputs: Int, dropout: Float = 0.5f) : SequentialBlock() {\n",
    "    val dense1 = Linear.builder().setUnits(mlpNumHiddens.toLong()).build()\n",
    "    val gelu: (NDList) -> NDList = Activation::gelu\n",
    "    val dropout1 = Dropout.builder().optRate(dropout).build()\n",
    "    val dense2 = Linear.builder().setUnits(mlpNumOutputs.toLong()).build()\n",
    "    val dropout2 = Dropout.builder().optRate(dropout).build()\n",
    "\n",
    "    init {\n",
    "        add(dense1)\n",
    "        add(gelu)\n",
    "        add(dropout1)\n",
    "        add(dense2)\n",
    "        add(dropout2)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea380b",
   "metadata": {},
   "source": [
    "The vision Transformer encoder block implementation\n",
    "just follows the pre-normalization design in :numref:`fig_vit`,\n",
    "where normalization is applied right *before* multi-head attention or the MLP.\n",
    "In contrast to post-normalization (\"add & norm\" in :numref:`fig_transformer`),\n",
    "where normalization is placed right *after* residual connections,\n",
    "pre-normalization leads to more effective or efficient training for Transformers :cite:`baevski2018adaptive,wang2019learning,xiong2020layer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d1b1387",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTBlock(numHiddens: Int, val normShape: Int, mlpNumHiddens: Int, numHeads: Int, dropout: Float, useBias: Boolean = false) : AbstractBlock() {\n",
    "    val ln1 = LayerNorm.builder().axis(-1).build()\n",
    "    val attention = MultiHeadAttention(numHiddens, numHeads, dropout, useBias)\n",
    "    val ln2 = LayerNorm.builder().axis(-1).build()\n",
    "    val mlp = ViTMLP(mlpNumHiddens, numHiddens, dropout)\n",
    "\n",
    "    override fun forwardInternal(\n",
    "        parameterStore: ParameterStore,\n",
    "        inputs: NDList,\n",
    "        training: Boolean,\n",
    "        params: PairList<String, Any>?\n",
    "    ): NDList {\n",
    "        val X0 = inputs[0]\n",
    "        val X1 = ln1.forward(parameterStore, NDList(X0), training, params).head()\n",
    "        val att = if (inputs.size < 2) {\n",
    "            attention.forward(parameterStore, NDList(X1, X1, X1), training, params).head()\n",
    "        } else {\n",
    "            attention.forward(parameterStore, NDList(X1, X1, X1, inputs[1]), training, params).head()\n",
    "        }\n",
    "        var X = X0.add(att)\n",
    "        val Y = ln2.forward(parameterStore, NDList(X), training, params).head()\n",
    "        val mlpOut = mlp.forward(parameterStore, NDList(Y), training, params).head()\n",
    "        X = X.add(mlpOut)\n",
    "        return NDList(X)\n",
    "    }\n",
    "\n",
    "    override fun initializeChildBlocks(manager: NDManager, dataType: DataType, vararg inputShapes: Shape) {\n",
    "        ln1.initialize(manager, dataType, inputShapes[0])\n",
    "        if (inputShapes.size > 1) {\n",
    "            attention.initialize(manager, dataType, inputShapes[0], inputShapes[0], inputShapes[0], inputShapes[1])\n",
    "        } else {\n",
    "            attention.initialize(manager, dataType, inputShapes[0], inputShapes[0], inputShapes[0])\n",
    "        }\n",
    "        ln2.initialize(manager, dataType, inputShapes[0])\n",
    "        mlp.initialize(manager, dataType, inputShapes[0])\n",
    "    }\n",
    "\n",
    "    /* We won't implement this since we won't be using it but it's required as part of an AbstractBlock  */\n",
    "    override fun getOutputShapes(inputShapes: Array<Shape>): Array<Shape> {\n",
    "        return inputShapes\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43e4837",
   "metadata": {},
   "source": [
    "Same as in :numref:`subsec_transformer-encoder`,\n",
    "any vision Transformer encoder block does not change its input shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0707549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDList size: 1\n",
      "0 : (2, 100, 24) float32\n",
      "\n",
      "Shapes : [(2, 100, 24)]\n"
     ]
    }
   ],
   "source": [
    "    val X1 = manager.ones(Shape(2, 100, 24))\n",
    "    val encoderBlk = ViTBlock(24, 24, 48, 8, 0.5f)\n",
    "    encoderBlk.initialize(manager, DataType.FLOAT32, X1.shape, Shape(2))\n",
    "    println(encoderBlk.forward(ps, NDList(X1, null), false))\n",
    "    println(\"Shapes : ${encoderBlk.getOutputShapes(arrayOf(X1.shape)).toList()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2d0ba",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "The forward pass of vision Transformers below is straightforward.\n",
    "First, input images are fed into an `PatchEmbedding` instance,\n",
    "whose output is concatenated with the \u201c&lt;cls&gt;\u201d  token embedding.\n",
    "They are summed with learnable positional embeddings before dropout.\n",
    "Then the output is fed into the Transformer encoder that stacks `num_blks` instances of the `ViTBlock` class.\n",
    "Finally, the representation of the \u201c&lt;cls&gt;\u201d  token is projected by the network head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b165132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(\n",
    "    val imgSize: Int,\n",
    "    patchSize: Int,\n",
    "    val numHiddens: Int,\n",
    "    mlpNumHiddens: Int,\n",
    "    numHeads: Int,\n",
    "    numBlks: Int,\n",
    "    embDropout: Float,\n",
    "    blkDropout: Float,\n",
    "    lr: Float = 0.1f,\n",
    "    useBias: Boolean = false,\n",
    "    val numClasses: Int = 10\n",
    ") : AbstractBlock() {\n",
    "    val patchEmbedding = PatchEmbedding(imgSize, patchSize, numHiddens)\n",
    "    val clsToken = Parameter.builder()\n",
    "        .optRequiresGrad(true)\n",
    "        .setType(Parameter.Type.BIAS)\n",
    "        .optShape(Shape(1, 1, numHiddens.toLong()))\n",
    "        .build()\n",
    "    val numSteps: Int = patchEmbedding.numPatches + 1\n",
    "    val posEmbedding = Parameter.builder()\n",
    "        .optRequiresGrad(true)\n",
    "        .optShape(Shape(1, numSteps.toLong(), numHiddens.toLong()))\n",
    "//    torch.randn(1, num_steps, num_hiddens))\n",
    "        .setType(Parameter.Type.BIAS)\n",
    "        .build()\n",
    "    val dropOut = Dropout.builder().optRate(embDropout).build()\n",
    "    val blks = mutableListOf<Pair<String, AbstractBlock>>()\n",
    "    val blks0 = SequentialBlock()\n",
    "    val head = SequentialBlock()\n",
    "        .add(LayerNorm.builder().build())\n",
    "        .add(Linear.builder().setUnits(numClasses.toLong()).build())\n",
    "    init {\n",
    "        addParameter(clsToken)\n",
    "        addParameter(posEmbedding)\n",
    "        clsToken.setInitializer(ConstantInitializer(0f))\n",
    "        for (i in 0 until numBlks) {\n",
    "            blks0.add(ViTBlock(numHiddens, numHiddens, mlpNumHiddens, numHeads, blkDropout, useBias))\n",
    "        }\n",
    "    }\n",
    "\n",
    "    override fun forwardInternal(\n",
    "        parameterStore: ParameterStore,\n",
    "        inputs: NDList,\n",
    "        training: Boolean,\n",
    "        params: PairList<String, Any>?\n",
    "    ): NDList {\n",
    "        var X = patchEmbedding.forward(parameterStore, inputs, training, params).head()\n",
    "        // X = torch.cat((self.cls_token.expand(X.shape[0], -1, -1), X), 1)\n",
    "\n",
    "        X = clsToken.array.repeat(0, X.shape[0]).concat(X, 1)\n",
    "        X = dropOut.forward(parameterStore, NDList(X.add(posEmbedding.array)), training, params).head()\n",
    "        X = blks0.forward(parameterStore, NDList(X), training, params).head()\n",
    "        return head.forward(parameterStore, NDList(X.get(NDIndex(\":, 0\"))), training, params)\n",
    "    }\n",
    "\n",
    "    override fun initializeChildBlocks(manager: NDManager, dataType: DataType, vararg inputShapes: Shape) {\n",
    "        clsToken.initialize(manager, dataType)\n",
    "        posEmbedding.initialize(manager, dataType)\n",
    "        patchEmbedding.initialize(manager, dataType, Shape(inputShapes[0][0], inputShapes[0][1], imgSize.toLong(), imgSize.toLong()))\n",
    "        blks0.initialize(manager, dataType, Shape(inputShapes[0][0], numSteps.toLong(), numHiddens.toLong()), Shape(inputShapes[0][0]))\n",
    "        head.initialize(manager, dataType, Shape(inputShapes[0][0], numHiddens.toLong()))\n",
    "    }\n",
    "\n",
    "    override fun getOutputShapes(inputShapes: Array<Shape>): Array<Shape> {\n",
    "        return arrayOf(Shape(inputShapes[0][0], numClasses.toLong()))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29130915",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training a vision Transformer on the Fashion-MNIST dataset is just like how CNNs were trained in :numref:`chap_modern_cnn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f12594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| Accuracy: 0.66, SoftmaxCrossEntropyLoss: 1.04\n",
      "Validating:  100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n",
      "Training:     68% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588            | Accuracy: 0.77, SoftmaxCrossEntropyLoss: 0.63"
     ]
    }
   ],
   "source": [
    "    val imgSize0 = 28\n",
    "    val patchSize0 = 16\n",
    "    val numHiddens0 = 512\n",
    "    val mlpNumHiddens0 = 2048\n",
    "    val numHeads0 = 8\n",
    "    val numBlks0 = 2\n",
    "    val embDropout0 = 0.1f\n",
    "    val blkDropout0 = 0.1f\n",
    "    val batchSize0 = 256\n",
    "    val lr = 0.001f\n",
    "    val X0 = manager.ones(Shape(batchSize0.toLong(), 1, imgSize0.toLong(), imgSize0.toLong()))\n",
    "    val encoder = ViT(imgSize0, patchSize0, numHiddens0, mlpNumHiddens0, numHeads0, numBlks0, embDropout0, blkDropout0, lr)\n",
    "//    encoder.initialize(manager, DataType.FLOAT32, X0.shape)\n",
    "\n",
    "    val randomShuffle = true\n",
    "\n",
    "// Get Training and Validation Datasets\n",
    "\n",
    "// Get Training and Validation Datasets\n",
    "    val trainingSet = FashionMnist.builder()\n",
    "        .optUsage(Dataset.Usage.TRAIN)\n",
    "        .setSampling(batchSize0, randomShuffle)\n",
    "        .optLimit(Long.MAX_VALUE)\n",
    "        .build()\n",
    "\n",
    "    val validationSet = FashionMnist.builder()\n",
    "        .optUsage(Dataset.Usage.TEST)\n",
    "        .setSampling(batchSize0, false)\n",
    "        .optLimit(Long.MAX_VALUE)\n",
    "        .build()\n",
    "    val model: Model = Model.newInstance(\"softmax-regression\")\n",
    "    model.setBlock(encoder)\n",
    "    val loss: Loss = Loss.softmaxCrossEntropyLoss()\n",
    "    val lrt: Tracker = Tracker.fixed(lr)\n",
    "    val adam: Optimizer = Optimizer.adam().optLearningRateTracker(lrt).build()\n",
    "    val config: DefaultTrainingConfig = DefaultTrainingConfig(loss)\n",
    "        .optOptimizer(adam) // Optimizer (loss function)\n",
    "        .optInitializer(XavierInitializer(), \"\")\n",
    "        .addEvaluator(Accuracy()) // Model Accuracy\n",
    "        .addTrainingListeners(*TrainingListener.Defaults.logging()); // Logging\n",
    "\n",
    "    val trainer: Trainer = model.newTrainer(config)\n",
    "    trainer.initialize(X0.shape)\n",
    "    trainer.metrics = Metrics()\n",
    "    EasyTrain.fit(trainer, 3, trainingSet, validationSet)\n",
    "\n",
    "    val batch = validationSet.getData(manager).iterator().next()\n",
    "    val X3 = batch.getData().head()\n",
    "    val yHat: IntArray = encoder.forward(ps, NDList(X3), false).head().argMax(1).toType(DataType.INT32, false).toIntArray()\n",
    "    println(yHat.toList().subList(0, 20))\n",
    "    println(batch.getLabels().head().toFloatArray().toList().subList(0, 20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0584f7",
   "metadata": {},
   "source": [
    "## Summary and Discussion\n",
    "\n",
    "You may notice that for small datasets like Fashion-MNIST, \n",
    "our implemented vision Transformer \n",
    "does not outperform the ResNet in :numref:`sec_resnet`.\n",
    "Similar observations can be made even on the ImageNet dataset (1.2 million images).\n",
    "This is because Transformers *lack* those useful principles in convolution, \n",
    "such as translation invariance and locality (:numref:`sec_why-conv`).\n",
    "However, the picture changes when training larger models on larger datasets (e.g., 300 million images),\n",
    "where vision Transformers outperform ResNets by a large margin in image classification, demonstrating\n",
    "intrinsic superiority of Transformers in scalability :cite:`Dosovitskiy.Beyer.Kolesnikov.ea.2021`.\n",
    "The introduction of vision Transformers \n",
    "has changed the landscape of network design for modeling image data.\n",
    "They were soon shown effective on the ImageNet dataset\n",
    "with data-efficient training strategies of DeiT :cite:`touvron2021training`.\n",
    "However, quadratic complexity of self-attention \n",
    "(:numref:`sec_self-attention-and-positional-encoding`)\n",
    "makes the Transformer architecture\n",
    "less suitable for higher-resolution images.\n",
    "Towards a general-purpose backbone network in computer vision,\n",
    "Swin Transformers addressed the quadratic computational complexity \n",
    "with respect to image size (:numref:`subsec_cnn-rnn-self-attention`)\n",
    "and added back convolution-like priors,\n",
    "extending the applicability of Transformers to a range of computer vision tasks \n",
    "beyond image classification with state-of-the-art results :cite:`liu2021swin`.\n",
    "\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. How does the value of `img_size` affect training time?\n",
    "1. Instead of projecting the \u201c&lt;cls&gt;\u201d token representation to the output, how to project the averaged patch representations? Implement this change and see how it affects the accuracy.\n",
    "1. Can you modify hyperparameters to improve the accuracy of the vision Transformer?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57870135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.8.0-dev-707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}