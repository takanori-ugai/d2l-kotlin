{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "194b3096",
   "metadata": {},
   "source": [
    "# The Transformer Architecture\n",
    ":label:`sec_transformer`\n",
    "\n",
    "\n",
    "We have compared CNNs, RNNs, and self-attention in\n",
    ":numref:`subsec_cnn-rnn-self-attention`.\n",
    "Notably, self-attention\n",
    "enjoys both parallel computation and\n",
    "the shortest maximum path length.\n",
    "Therefore naturally,\n",
    "it is appealing to design deep architectures\n",
    "by using self-attention.\n",
    "Unlike earlier self-attention models\n",
    "that still rely on RNNs for input representations :cite:`Cheng.Dong.Lapata.2016,Lin.Feng.Santos.ea.2017,Paulus.Xiong.Socher.2017`,\n",
    "the Transformer model\n",
    "is solely based on attention mechanisms\n",
    "without any convolutional or recurrent layer :cite:`Vaswani.Shazeer.Parmar.ea.2017`.\n",
    "Though originally proposed\n",
    "for sequence to sequence learning on text data,\n",
    "Transformers have been\n",
    "pervasive in a wide range of\n",
    "modern deep learning applications,\n",
    "such as in areas of language, vision, speech, and reinforcement learning.\n",
    "\n",
    "## Model\n",
    "\n",
    "As an instance of the encoder-decoder\n",
    "architecture,\n",
    "the overall architecture of\n",
    "the Transformer\n",
    "is presented in :numref:`fig_transformer`.\n",
    "As we can see,\n",
    "the Transformer is composed of an encoder and a decoder.\n",
    "Different from\n",
    "Bahdanau attention\n",
    "for sequence to sequence learning\n",
    "in :numref:`fig_s2s_attention_details`,\n",
    "the input (source) and output (target)\n",
    "sequence embeddings\n",
    "are added with positional encoding\n",
    "before being fed into\n",
    "the encoder and the decoder\n",
    "that stack modules based on self-attention.\n",
    "\n",
    "![The Transformer architecture.](https://d2l.ai/_images/transformer.svg)\n",
    ":width:`400px`\n",
    ":label:`fig_transformer`\n",
    "\n",
    "\n",
    "Now we provide an overview of the\n",
    "Transformer architecture in :numref:`fig_transformer`.\n",
    "On a high level,\n",
    "the Transformer encoder is a stack of multiple identical layers,\n",
    "where each layer\n",
    "has two sublayers (either is denoted as $\\mathrm{sublayer}$).\n",
    "The first\n",
    "is a multi-head self-attention pooling\n",
    "and the second is a positionwise feed-forward network.\n",
    "Specifically,\n",
    "in the encoder self-attention,\n",
    "queries, keys, and values are all from the\n",
    "outputs of the previous encoder layer.\n",
    "Inspired by the ResNet design in :numref:`sec_resnet`,\n",
    "a residual connection is employed\n",
    "around both sublayers.\n",
    "In the Transformer,\n",
    "for any input $\\mathbf{x} \\in \\mathbb{R}^d$ at any position of the sequence,\n",
    "we require that $\\mathrm{sublayer}(\\mathbf{x}) \\in \\mathbb{R}^d$ so that\n",
    "the residual connection $\\mathbf{x} + \\mathrm{sublayer}(\\mathbf{x}) \\in \\mathbb{R}^d$ is feasible.\n",
    "This addition from the residual connection is immediately\n",
    "followed by layer normalization :cite:`Ba.Kiros.Hinton.2016`.\n",
    "As a result, the Transformer encoder outputs a $d$-dimensional vector representation \n",
    "for each position of the input sequence.\n",
    "\n",
    "The Transformer decoder is also a stack of multiple identical layers \n",
    "with residual connections and layer normalizations.\n",
    "Besides the two sublayers described in\n",
    "the encoder, the decoder inserts\n",
    "a third sublayer, known as\n",
    "the encoder-decoder attention,\n",
    "between these two.\n",
    "In the encoder-decoder attention,\n",
    "queries are from the\n",
    "outputs of the previous decoder layer,\n",
    "and the keys and values are\n",
    "from the Transformer encoder outputs.\n",
    "In the decoder self-attention,\n",
    "queries, keys, and values are all from the\n",
    "outputs of the previous decoder layer.\n",
    "However, each position in the decoder is\n",
    "allowed to only attend to all positions in the decoder\n",
    "up to that position.\n",
    "This *masked* attention\n",
    "preserves the auto-regressive property,\n",
    "ensuring that the prediction only depends \n",
    "on those output tokens that have been generated.\n",
    "\n",
    "\n",
    "We have already described and implemented\n",
    "multi-head attention based on scaled dot-products\n",
    "in :numref:`sec_multihead-attention`\n",
    "and positional encoding in :numref:`subsec_positional-encoding`.\n",
    "In the following, we will implement\n",
    "the rest of the Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7d02837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"salZB4\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "       if(!window.letsPlotCallQueue) {\n",
       "           window.letsPlotCallQueue = [];\n",
       "       }; \n",
       "       window.letsPlotCall = function(f) {\n",
       "           window.letsPlotCallQueue.push(f);\n",
       "       };\n",
       "       (function() {\n",
       "           var script = document.createElement(\"script\");\n",
       "           script.type = \"text/javascript\";\n",
       "           script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v4.8.2/js-package/distr/lets-plot.min.js\";\n",
       "           script.onload = function() {\n",
       "               window.letsPlotCall = function(f) {f();};\n",
       "               window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "               window.letsPlotCallQueue = [];\n",
       "               \n",
       "               \n",
       "           };\n",
       "           script.onerror = function(event) {\n",
       "               window.letsPlotCall = function(f) {};\n",
       "               window.letsPlotCallQueue = [];\n",
       "               var div = document.createElement(\"div\");\n",
       "               div.style.color = 'darkred';\n",
       "               div.textContent = 'Error loading Lets-Plot JS';\n",
       "               document.getElementById(\"salZB4\").appendChild(div);\n",
       "           };\n",
       "           var e = document.getElementById(\"salZB4\");\n",
       "           e.appendChild(script);\n",
       "       })();\n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "            <div id=\"kotlin_out_0\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                            if(!window.kotlinQueues) {\n",
       "                window.kotlinQueues = {};\n",
       "            }\n",
       "            if(!window.kotlinQueues[\"letsPlotJs\"]) {\n",
       "                var resQueue = [];\n",
       "                window.kotlinQueues[\"letsPlotJs\"] = resQueue;\n",
       "                window[\"call_letsPlotJs\"] = function(f) {\n",
       "                    resQueue.push(f);\n",
       "                }\n",
       "            }\n",
       "            (function (){\n",
       "                var modifiers = [(function(script) {\n",
       "    script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v4.8.2/js-package/distr/lets-plot.min.js\"\n",
       "    script.type = \"text/javascript\";\n",
       "})];\n",
       "                var e = document.getElementById(\"kotlin_out_0\");\n",
       "                modifiers.forEach(function (gen) {\n",
       "                    var script = document.createElement(\"script\");\n",
       "                    gen(script)\n",
       "                    script.addEventListener(\"load\", function() {\n",
       "                        window[\"call_letsPlotJs\"] = function(f) {f();};\n",
       "                        window.kotlinQueues[\"letsPlotJs\"].forEach(function(f) {f();});\n",
       "                        window.kotlinQueues[\"letsPlotJs\"] = [];\n",
       "                    }, false);\n",
       "                    script.addEventListener(\"error\", function() {\n",
       "                        window[\"call_letsPlotJs\"] = function(f) {};\n",
       "                        window.kotlinQueues[\"letsPlotJs\"] = [];\n",
       "                        var div = document.createElement(\"div\");\n",
       "                        div.style.color = 'darkred';\n",
       "                        div.textContent = 'Error loading resource letsPlotJs';\n",
       "                        document.getElementById(\"kotlin_out_0\").appendChild(div);\n",
       "                    }, false);\n",
       "                    \n",
       "                    e.appendChild(script);\n",
       "                });\n",
       "            })();\n",
       "            </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%use @file[../djl-pytorch.json]\n",
    "%use lets-plot\n",
    "@file:DependsOn(\"../D2J-1.0-SNAPSHOT.jar\")\n",
    "import ai.djl.modality.nlp.DefaultVocabulary\n",
    "import ai.djl.modality.nlp.Vocabulary\n",
    "import ai.djl.modality.nlp.embedding.TrainableWordEmbedding\n",
    "import jp.live.ugai.d2j.timemachine.RNNModelScratch\n",
    "import jp.live.ugai.d2j.timemachine.TimeMachine.trainCh8\n",
    "import jp.live.ugai.d2j.timemachine.TimeMachineDataset\n",
    "import jp.live.ugai.d2j.timemachine.Vocab\n",
    "import jp.live.ugai.d2j.RNNModel\n",
    "import jp.live.ugai.d2j.util.StopWatch\n",
    "import jp.live.ugai.d2j.util.Accumulator\n",
    "import jp.live.ugai.d2j.util.Training\n",
    "import jp.live.ugai.d2j.util.TrainingChapter9\n",
    "import jp.live.ugai.d2j.lstm.Decoder\n",
    "import jp.live.ugai.d2j.lstm.Encoder\n",
    "import jp.live.ugai.d2j.lstm.EncoderDecoder\n",
    "import jp.live.ugai.d2j.util.NMT\n",
    "import jp.live.ugai.d2j.attention.AttentionDecoder\n",
    "import jp.live.ugai.d2j.PositionalEncoding\n",
    "import jp.live.ugai.d2j.MaskedSoftmaxCELoss\n",
    "import java.util.Locale\n",
    "import kotlin.random.Random\n",
    "import kotlin.collections.List\n",
    "import kotlin.collections.Map\n",
    "import kotlin.Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1acf9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "val manager = NDManager.newBaseManager()\n",
    "val ps = ParameterStore(manager, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67078d09",
   "metadata": {},
   "source": [
    "## [**Positionwise Feed-Forward Networks**]\n",
    ":label:`subsec_positionwise-ffn`\n",
    "\n",
    "The positionwise feed-forward network transforms\n",
    "the representation at all the sequence positions\n",
    "using the same MLP.\n",
    "This is why we call it *positionwise*.\n",
    "In the implementation below,\n",
    "the input `X` with shape\n",
    "(batch size, number of time steps or sequence length in tokens,\n",
    "number of hidden units or feature dimension)\n",
    "will be transformed by a two-layer MLP into\n",
    "an output tensor of shape\n",
    "(batch size, number of time steps, `ffn_num_outputs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8fb067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fun positionWiseFFN(ffn_num_hiddens: Long, ffn_num_outputs: Long) : AbstractBlock {\n",
    "    val net = SequentialBlock()\n",
    "    net.add(Linear.builder().setUnits(ffn_num_hiddens).build())\n",
    "    net.add(Activation::relu)\n",
    "    net.add(Linear.builder().setUnits(ffn_num_outputs).build());\n",
    "//    net.setInitializer(NormalInitializer(), Parameter.Type.WEIGHT)\n",
    "    return net\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe05fd7",
   "metadata": {},
   "source": [
    "The following example\n",
    "shows that [**the innermost dimension\n",
    "of a tensor changes**] to\n",
    "the number of outputs in\n",
    "the positionwise feed-forward network.\n",
    "Since the same MLP transforms\n",
    "at all the positions,\n",
    "when the inputs at all these positions are the same,\n",
    "their outputs are also identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "344cebd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ND: (3, 8) gpu(0) float32\n",
       "Check the \"Development Guideline\"->Debug to enable array display.\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ffn = positionWiseFFN(4, 8)\n",
    "ffn.initialize(manager, DataType.FLOAT32, Shape(2,3,4))\n",
    "ffn.forward(ps, NDList(manager.ones(Shape(2,3,4))), false)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e927be",
   "metadata": {},
   "source": [
    "## Residual Connection and Layer Normalization\n",
    "\n",
    "Now let's focus on the \"add & norm\" component in :numref:`fig_transformer`.\n",
    "As we described at the beginning of this section,\n",
    "this is a residual connection immediately\n",
    "followed by layer normalization.\n",
    "Both are key to effective deep architectures.\n",
    "\n",
    "In :numref:`sec_batch_norm`,\n",
    "we explained how batch normalization\n",
    "recenters and rescales across the examples within\n",
    "a minibatch.\n",
    "As discussed in :numref:`subsec_layer-normalization-in-bn`,\n",
    "layer normalization is the same as batch normalization\n",
    "except that the former\n",
    "normalizes across the feature dimension,\n",
    "thus enjoying benefits of scale independence and batch size independence.\n",
    "Despite its pervasive applications\n",
    "in computer vision,\n",
    "batch normalization\n",
    "is usually empirically\n",
    "less effective than layer normalization\n",
    "in natural language processing\n",
    "tasks, whose inputs are often\n",
    "variable-length sequences.\n",
    "\n",
    "The following code snippet\n",
    "[**compares the normalization across different dimensions\n",
    "by layer normalization and batch normalization**].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3450c8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm: ND: (2, 2) gpu(0) float32\n",
      "Check the \"Development Guideline\"->Debug to enable array display.\n",
      "\n",
      "BatchNorm: ND: (2, 2) gpu(0) float32\n",
      "Check the \"Development Guideline\"->Debug to enable array display.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val ln = LayerNorm.builder().build()\n",
    "ln.initialize(manager, DataType.FLOAT32, Shape(2,2))\n",
    "val bn = BatchNorm.builder().build()\n",
    "bn.initialize(manager, DataType.FLOAT32, Shape(2,2))\n",
    "val X = manager.create(floatArrayOf(1f,2f,2f,3f)).reshape(Shape(2,2))\n",
    "print(\"LayerNorm: \")\n",
    "println(ln.forward(ps, NDList(X), false)[0])\n",
    "print(\"BatchNorm: \")\n",
    "println(bn.forward(ps, NDList(X), false)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae5240",
   "metadata": {},
   "source": [
    "Now we can implement the `AddNorm` class\n",
    "[**using a residual connection followed by layer normalization**].\n",
    "Dropout is also applied for regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578b7ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    class AddNorm(rate: Float) : AbstractBlock() {\n",
    "        val dropout = Dropout.builder().optRate(rate).build()\n",
    "        val ln = LayerNorm.builder().build()\n",
    "\n",
    "        init {\n",
    "            addChildBlock(\"dropout\", dropout)\n",
    "            addChildBlock(\"layerNorm\", ln)\n",
    "        }\n",
    "\n",
    "        override fun forwardInternal(\n",
    "            ps: ParameterStore,\n",
    "            inputs: NDList,\n",
    "            training: Boolean,\n",
    "            params: PairList<String, Any>?\n",
    "        ): NDList {\n",
    "            val x = inputs[0]\n",
    "            val y = inputs[1]\n",
    "            val dropoutResult = dropout.forward(ps, NDList(y), training, params).singletonOrThrow()\n",
    "            val result = ln.forward(ps, NDList(dropoutResult.add(x)), training, params)\n",
    "            return result\n",
    "        }\n",
    "\n",
    "        override fun getOutputShapes(inputShapes: Array<Shape>): Array<Shape> {\n",
    "            return inputShapes\n",
    "        }\n",
    "\n",
    "        override fun initializeChildBlocks(manager: NDManager, dataType: DataType, vararg inputShapes: Shape) {\n",
    "            dropout.initialize(manager, dataType, *inputShapes)\n",
    "            ln.initialize(manager, dataType, *inputShapes)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    fun maskedSoftmax(_X: NDArray, _validLens: NDArray?): NDArray {\n",
    "        var validLens = _validLens ?: return _X.softmax(-1)\n",
    "        // Align validLens to the same device as X (e.g., GPU)\n",
    "        if (validLens.device != _X.device) {\n",
    "            validLens = validLens.toDevice(_X.device, false)\n",
    "        }\n",
    "        val shape: Shape = _X.shape\n",
    "        val lastDim = shape.get(shape.dimension() - 1)\n",
    "        // validLens can be 1D (batch,) or 2D (batch, steps); flatten to match X reshaping\n",
    "        var lens = if (validLens.dataType == DataType.FLOAT32) validLens else validLens.toType(DataType.FLOAT32, false)\n",
    "        lens = if (lens.shape.dimension() == 1) {\n",
    "            lens.repeat(shape.get(1))\n",
    "        } else {\n",
    "            lens.reshape(-1)\n",
    "        }\n",
    "        val X2 = _X.reshape(Shape(-1, lastDim))\n",
    "        // If lens shape does not match, fall back to unmasked softmax\n",
    "        if (lens.shape.size() != X2.shape.get(0)) {\n",
    "            if (lens !== validLens) {\n",
    "                lens.close()\n",
    "            }\n",
    "            return _X.softmax(-1)\n",
    "        }\n",
    "        // Build mask on the last dimension and apply a large negative bias\n",
    "        val arange = _X.manager.arange(lastDim.toFloat()).reshape(1, -1)\n",
    "        val mask = arange.lt(lens.reshape(-1, 1))\n",
    "        val maskFloat = mask.toType(_X.dataType, false)\n",
    "        val invMaskFloat = mask.logicalNot().toType(_X.dataType, false)\n",
    "        val maskedInput = X2.mul(maskFloat).add(invMaskFloat.mul(-1.0E6F))\n",
    "        val out = maskedInput.softmax(-1).reshape(shape)\n",
    "        arange.close()\n",
    "        mask.close()\n",
    "        maskFloat.close()\n",
    "        invMaskFloat.close()\n",
    "        maskedInput.close()\n",
    "        if (lens !== validLens) {\n",
    "            lens.close()\n",
    "        }\n",
    "        return out\n",
    "    }\n",
    "\n",
    "    fun transposeQkv(_X: NDArray, numHeads: Int): NDArray {\n",
    "        var X = _X\n",
    "        X = X.reshape(X.shape[0], X.shape[1], numHeads.toLong(), -1)\n",
    "        X = X.transpose(0, 2, 1, 3)\n",
    "        return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "    }\n",
    "\n",
    "    fun transposeOutput(_X: NDArray, numHeads: Int): NDArray {\n",
    "        var X = _X\n",
    "        X = X.reshape(-1, numHeads.toLong(), X.shape[1], X.shape[2])\n",
    "        X = X.transpose(0, 2, 1, 3)\n",
    "        return X.reshape(X.shape[0], X.shape[1], -1)\n",
    "    }\n",
    "\n",
    "    class DotProductAttention(dropout: Float) : AbstractBlock() {\n",
    "        private val dropout: Dropout\n",
    "        var attentionWeights: NDArray? = null\n",
    "        private var outputShapes: Array<Shape> = arrayOf<Shape>()\n",
    "\n",
    "        init {\n",
    "            this.dropout = Dropout.builder().optRate(dropout).build()\n",
    "            addChildBlock(\"dropout\", this.dropout)\n",
    "        }\n",
    "\n",
    "        override fun forwardInternal(\n",
    "            ps: ParameterStore,\n",
    "            inputs: NDList,\n",
    "            training: Boolean,\n",
    "            params: PairList<String, Any>?\n",
    "        ): NDList {\n",
    "            val queries = inputs[0]\n",
    "            val keys = inputs[1]\n",
    "            val values = inputs[2]\n",
    "            val validLens = if (inputs.size > 3) inputs[3] else null\n",
    "            val d = keys.shape.get(keys.shape.dimension() - 1).toDouble()\n",
    "            val scores = queries.matMul(keys.swapAxes(1, 2)).div(Math.sqrt(d))\n",
    "            attentionWeights = maskedSoftmax(scores, validLens)\n",
    "            val result = dropout.forward(ps, NDList(attentionWeights), training, params)\n",
    "            return NDList(result[0].matMul(values))\n",
    "        }\n",
    "\n",
    "        override fun getOutputShapes(inputShapes: Array<Shape>): Array<Shape> {\n",
    "            return outputShapes\n",
    "        }\n",
    "\n",
    "        override fun initializeChildBlocks(manager: NDManager, dataType: DataType, vararg inputShapes: Shape) {\n",
    "            val batchSize = inputShapes[0].get(0)\n",
    "            val numQueries = inputShapes[0].get(1)\n",
    "            val numKvs = inputShapes[1].get(1)\n",
    "            val scoresShape = Shape(batchSize, numQueries, numKvs)\n",
    "            dropout.initialize(manager, dataType, scoresShape)\n",
    "            outputShapes = dropout.getOutputShapes(arrayOf(scoresShape))\n",
    "        }\n",
    "    }\n",
    "\n",
    "    class MultiHeadAttention(numHiddens: Int, private val numHeads: Int, dropout: Float, useBias: Boolean) :\n",
    "        AbstractBlock() {\n",
    "        var attention: DotProductAttention\n",
    "        private val W_k: Linear\n",
    "        private val W_q: Linear\n",
    "        private val W_v: Linear\n",
    "        private val W_o: Linear\n",
    "\n",
    "        init {\n",
    "            attention = DotProductAttention(dropout)\n",
    "            W_q = Linear.builder().setUnits(numHiddens.toLong()).optBias(useBias).build()\n",
    "            addChildBlock(\"W_q\", W_q)\n",
    "            W_k = Linear.builder().setUnits(numHiddens.toLong()).optBias(useBias).build()\n",
    "            addChildBlock(\"W_k\", W_k)\n",
    "            W_v = Linear.builder().setUnits(numHiddens.toLong()).optBias(useBias).build()\n",
    "            addChildBlock(\"W_v\", W_v)\n",
    "            W_o = Linear.builder().setUnits(numHiddens.toLong()).optBias(useBias).build()\n",
    "            addChildBlock(\"W_o\", W_o)\n",
    "            val dropout1 = Dropout.builder().optRate(dropout).build()\n",
    "            addChildBlock(\"dropout\", dropout1)\n",
    "        }\n",
    "\n",
    "        override fun forwardInternal(\n",
    "            ps: ParameterStore,\n",
    "            inputs: NDList,\n",
    "            training: Boolean,\n",
    "            params: PairList<String, Any>?\n",
    "        ): NDList {\n",
    "            var queries = inputs[0]\n",
    "            var keys = inputs[1]\n",
    "            var values = inputs[2]\n",
    "            val validLens = if (inputs.size > 3) inputs[3] else null\n",
    "            val expandedValidLens = validLens?.repeat(0, numHeads.toLong())\n",
    "            queries = transposeQkv(W_q.forward(ps, NDList(queries), training, params)[0], numHeads)\n",
    "            keys = transposeQkv(W_k.forward(ps, NDList(keys), training, params)[0], numHeads)\n",
    "            values = transposeQkv(W_v.forward(ps, NDList(values), training, params)[0], numHeads)\n",
    "            val attnInputs = if (expandedValidLens == null) {\n",
    "                NDList(queries, keys, values)\n",
    "            } else {\n",
    "                NDList(queries, keys, values, expandedValidLens)\n",
    "            }\n",
    "            val output = attention.forward(ps, attnInputs, training, params)[0]\n",
    "            val outputConcat = transposeOutput(output, numHeads)\n",
    "            return NDList(W_o.forward(ps, NDList(outputConcat), training, params)[0])\n",
    "        }\n",
    "\n",
    "        override fun getOutputShapes(inputShapes: Array<Shape>): Array<Shape> {\n",
    "            throw UnsupportedOperationException(\"Not implemented\")\n",
    "        }\n",
    "\n",
    "        override fun initializeChildBlocks(manager: NDManager, dataType: DataType, vararg inputShapes: Shape) {\n",
    "            val sub = manager.newSubManager()\n",
    "            var queries = sub.zeros(inputShapes[0], dataType)\n",
    "            var keys = sub.zeros(inputShapes[1], dataType)\n",
    "            var values = sub.zeros(inputShapes[2], dataType)\n",
    "            var validLens = sub.zeros(inputShapes[3], dataType)\n",
    "            validLens = validLens.repeat(0, numHeads.toLong())\n",
    "\n",
    "            val ps = ParameterStore(sub, false)\n",
    "\n",
    "            W_q.initialize(manager, dataType, queries.shape)\n",
    "            W_k.initialize(manager, dataType, keys.shape)\n",
    "            W_v.initialize(manager, dataType, values.shape)\n",
    "\n",
    "            queries = transposeQkv(W_q.forward(ps, NDList(queries), false)[0], numHeads)\n",
    "            keys = transposeQkv(W_k.forward(ps, NDList(keys), false)[0], numHeads)\n",
    "            values = transposeQkv(W_v.forward(ps, NDList(values), false)[0], numHeads)\n",
    "\n",
    "            val list = NDList(queries, keys, values, validLens)\n",
    "            attention.initialize(sub, dataType, *list.shapes)\n",
    "            val output = attention.forward(ps, list, false)[0]\n",
    "            val outputConcat = transposeOutput(output, numHeads)\n",
    "            W_o.initialize(manager, dataType, outputConcat.shape)\n",
    "            sub.close()\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6593bf5",
   "metadata": {},
   "source": [
    "The residual connection requires that\n",
    "the two inputs are of the same shape\n",
    "so that [**the output tensor also has the same shape after the addition operation**].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566595a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val addNorm = AddNorm(0.5f)\n",
    "addNorm.initialize(manager, DataType.FLOAT32, Shape(1,4))\n",
    "addNorm.forward(ps, NDList(manager.ones(Shape(2,3,4)), manager.ones(Shape(2,3,4))), false)[0].shapeEquals(manager.ones(Shape(2,3,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca00c993",
   "metadata": {},
   "source": [
    "## Encoder\n",
    ":label:`subsec_transformer-encoder`\n",
    "\n",
    "With all the essential components to assemble\n",
    "the Transformer encoder,\n",
    "let's start by\n",
    "implementing [**a single layer within the encoder**].\n",
    "The following `TransformerEncoderBlock` class\n",
    "contains two sublayers: multi-head self-attention and positionwise feed-forward networks,\n",
    "where a residual connection followed by layer normalization is employed\n",
    "around both sublayers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4859fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    class TransformerEncoderBlock(\n",
    "        numHiddens: Int,\n",
    "        ffnNumHiddens: Long,\n",
    "        numHeads: Int,\n",
    "        dropout: Float,\n",
    "        useBias: Boolean = false\n",
    "    ) : AbstractBlock() {\n",
    "        val attention = MultiHeadAttention(numHiddens, numHeads, dropout, useBias)\n",
    "        val addnorm1 = AddNorm(dropout)\n",
    "        val ffn = positionWiseFFN(ffnNumHiddens, numHiddens.toLong())\n",
    "        val addnorm2 = AddNorm(dropout)\n",
    "        init {\n",
    "            addChildBlock(\"attention\", attention)\n",
    "            addChildBlock(\"addnorm1\", addnorm1)\n",
    "            addChildBlock(\"ffn\", ffn)\n",
    "            addChildBlock(\"addnorm2\", addnorm2)\n",
    "        }\n",
    "\n",
    "        override fun forwardInternal(\n",
    "            ps: ParameterStore,\n",
    "            inputs: NDList,\n",
    "            training: Boolean,\n",
    "            params: PairList<String, Any>?\n",
    "        ): NDList {\n",
    "            val x = inputs[0]\n",
    "            val validLens = inputs[1]\n",
    "            val y = addnorm1.forward(ps, NDList(x, attention.forward(ps, NDList(x, x, x, validLens), training, params).singletonOrThrow()), training, params)\n",
    "            val ret = addnorm2.forward(ps, NDList(y.singletonOrThrow(), ffn.forward(ps, y, training, params).singletonOrThrow()), training, params)\n",
    "            return ret\n",
    "        }\n",
    "\n",
    "        override fun getOutputShapes(inputShapes: Array<Shape>): Array<Shape> {\n",
    "            return arrayOf<Shape>()\n",
    "        }\n",
    "\n",
    "        override fun initializeChildBlocks(manager: NDManager, dataType: DataType, vararg inputShapes: Shape) {\n",
    "            val shapes = arrayOf(inputShapes[0], inputShapes[0], inputShapes[0], inputShapes[1])\n",
    "            attention.initialize(manager, dataType, *shapes)\n",
    "            addnorm1.initialize(manager, dataType, inputShapes[0])\n",
    "            ffn.initialize(manager, dataType, inputShapes[0])\n",
    "            addnorm2.initialize(manager, dataType, inputShapes[0])\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6055bfe3",
   "metadata": {},
   "source": [
    "As we can see,\n",
    "[**any layer in the Transformer encoder\n",
    "does not change the shape of its input.**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3004a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val X = manager.ones(Shape(2, 100, 24))\n",
    "val validLens = manager.create(floatArrayOf(3f,2f))\n",
    "val encoderBlock = TransformerEncoderBlock(24,48,8, 0.5f)\n",
    "encoderBlock.initialize(manager, DataType.FLOAT32, X.shape, validLens.shape)\n",
    "encoderBlock.forward(ps, NDList(X, validLens), false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071dcb35",
   "metadata": {},
   "source": [
    "In the following [**Transformer encoder**] implementation,\n",
    "we stack `num_blks` instances of the above `TransformerEncoderBlock` classes.\n",
    "Since we use the fixed positional encoding\n",
    "whose values are always between -1 and 1,\n",
    "we multiply values of the learnable input embeddings\n",
    "by the square root of the embedding dimension\n",
    "to rescale before summing up the input embedding and the positional encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7392ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "    class TransformerEncoder(\n",
    "        vocabSize: Int,\n",
    "        val numHiddens: Int,\n",
    "        ffnNumHiddens: Long,\n",
    "        numHeads: Long,\n",
    "        numBlks: Int,\n",
    "        dropout: Float,\n",
    "        useBias: Boolean = false\n",
    "    ) : Encoder() {\n",
    "\n",
    "        private val embedding: TrainableWordEmbedding\n",
    "        val posEncoding = PositionalEncoding(numHiddens, dropout, 1000, manager)\n",
    "        val blks = mutableListOf<TransformerEncoderBlock>()\n",
    "        val attentionWeights = Array<NDArray?>(numBlks) { null }\n",
    "\n",
    "        /* The RNN encoder for sequence to sequence learning. */\n",
    "        init {\n",
    "            val list: List<String> = (0 until vocabSize).map { it.toString() }\n",
    "            val vocab: Vocabulary = DefaultVocabulary(list)\n",
    "            // Embedding layer\n",
    "            embedding = TrainableWordEmbedding.builder()\n",
    "                .optNumEmbeddings(vocabSize)\n",
    "                .setEmbeddingSize(numHiddens)\n",
    "                .setVocabulary(vocab)\n",
    "                .build()\n",
    "            addChildBlock(\"embedding\", embedding)\n",
    "            repeat(numBlks) {\n",
    "                blks.add(TransformerEncoderBlock(numHiddens, ffnNumHiddens, numHeads.toInt(), dropout, useBias))\n",
    "            }\n",
    "        }\n",
    "\n",
    "        override fun forwardInternal(\n",
    "            ps: ParameterStore,\n",
    "            inputs: NDList,\n",
    "            training: Boolean,\n",
    "            params: PairList<String, Any>?\n",
    "        ): NDList {\n",
    "            var X = inputs[0]\n",
    "            X = X.toType(DataType.INT64, false)\n",
    "            val validLens = inputs[1]\n",
    "            val emb = embedding.forward(ps, NDList(X), training, params).singletonOrThrow().mul(Math.sqrt(numHiddens.toDouble()))\n",
    "            X = posEncoding.forward(ps, NDList(emb), training, params).singletonOrThrow()\n",
    "            for (i in 0 until blks.size) {\n",
    "                X = blks[i].forward(ps, NDList(X, validLens), training, params).singletonOrThrow()\n",
    "                attentionWeights[i] = blks[i].attention.attention.attentionWeights\n",
    "            }\n",
    "            return NDList(X, validLens)\n",
    "        }\n",
    "\n",
    "        override fun initializeChildBlocks(manager: NDManager, dataType: DataType, vararg inputShapes: Shape) {\n",
    "            embedding.initialize(manager, dataType, *inputShapes)\n",
    "            for (blk in blks) {\n",
    "                blk.initialize(manager, dataType, inputShapes[0].add(numHiddens.toLong()), inputShapes[1])\n",
    "            }\n",
    "        }\n",
    "\n",
    "        override fun getOutputShapes(inputShapes: Array<Shape>): Array<Shape> {\n",
    "            return arrayOf(inputShapes[0].add(numHiddens.toLong()), inputShapes[1])\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acc2691",
   "metadata": {},
   "source": [
    "Below we specify hyperparameters to [**create a two-layer Transformer encoder**].\n",
    "The shape of the Transformer encoder output\n",
    "is (batch size, number of time steps, `num_hiddens`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89433dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val encoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5f)\n",
    "encoder.initialize(manager, DataType.FLOAT32, Shape(2,100), validLens.shape)\n",
    "encoder.forward(ps, NDList(manager.ones(Shape(2, 100)), validLens), false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aec397f",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "As shown in :numref:`fig_transformer`,\n",
    "[**the Transformer decoder\n",
    "is composed of multiple identical layers**].\n",
    "Each layer is implemented in the following\n",
    "`TransformerDecoderBlock` class,\n",
    "which contains three sublayers:\n",
    "decoder self-attention,\n",
    "encoder-decoder attention,\n",
    "and positionwise feed-forward networks.\n",
    "These sublayers employ\n",
    "a residual connection around them\n",
    "followed by layer normalization.\n",
    "\n",
    "\n",
    "As we described earlier in this section,\n",
    "in the masked multi-head decoder self-attention\n",
    "(the first sublayer),\n",
    "queries, keys, and values\n",
    "all come from the outputs of the previous decoder layer.\n",
    "When training sequence-to-sequence models,\n",
    "tokens at all the positions (time steps)\n",
    "of the output sequence\n",
    "are known.\n",
    "However,\n",
    "during prediction\n",
    "the output sequence is generated token by token;\n",
    "thus,\n",
    "at any decoder time step\n",
    "only the generated tokens\n",
    "can be used in the decoder self-attention.\n",
    "To preserve auto-regression in the decoder,\n",
    "its masked self-attention\n",
    "specifies  `dec_valid_lens` so that\n",
    "any query\n",
    "only attends to\n",
    "all positions in the decoder\n",
    "up to the query position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df41f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    class TransformerDecoderBlock(\n",
    "        val numHiddens: Int,\n",
    "        ffnNumHiddens: Long,\n",
    "        numHeads: Long,\n",
    "        dropout: Float,\n",
    "        _i: Int\n",
    "    ) : AbstractBlock() {\n",
    "        val i = _i\n",
    "        val attention1 = MultiHeadAttention(numHiddens, numHeads.toInt(), dropout, false)\n",
    "        val addnorm1 = AddNorm(dropout)\n",
    "        val attention2 = MultiHeadAttention(numHiddens, numHeads.toInt(), dropout, false)\n",
    "        val addnorm2 = AddNorm(dropout)\n",
    "        val ffn = positionWiseFFN(ffnNumHiddens, numHiddens.toLong())\n",
    "        val addnorm3 = AddNorm(dropout)\n",
    "\n",
    "        init {\n",
    "            addChildBlock(\"attention1\", attention1)\n",
    "            addChildBlock(\"addnorm1\", addnorm1)\n",
    "            addChildBlock(\"attention2\", attention2)\n",
    "            addChildBlock(\"addnorm2\", addnorm2)\n",
    "            addChildBlock(\"ffn\", ffn)\n",
    "            addChildBlock(\"addnorm3\", addnorm3)\n",
    "        }\n",
    "\n",
    "        override fun initializeChildBlocks(manager: NDManager, dataType: DataType, vararg inputShapes: Shape) {\n",
    "            val decShape = inputShapes[0]\n",
    "            val decValidLensShape = Shape(decShape.get(0), decShape.get(1))\n",
    "            val encOutputsShape = if (inputShapes.size > 1 && inputShapes[1].dimension() == 3) {\n",
    "                inputShapes[1]\n",
    "            } else {\n",
    "                decShape\n",
    "            }\n",
    "            val encValidLensShape = if (inputShapes.size > 2) {\n",
    "                inputShapes[2]\n",
    "            } else {\n",
    "                Shape(decShape.get(0))\n",
    "            }\n",
    "            attention1.initialize(manager, dataType, decShape, decShape, decShape, decValidLensShape)\n",
    "            addnorm1.initialize(manager, dataType, decShape)\n",
    "            attention2.initialize(manager, dataType, decShape, encOutputsShape, encOutputsShape, encValidLensShape)\n",
    "            addnorm2.initialize(manager, dataType, decShape)\n",
    "            ffn.initialize(manager, dataType, decShape)\n",
    "            addnorm3.initialize(manager, dataType, decShape)\n",
    "        }\n",
    "        override fun forwardInternal(\n",
    "            ps: ParameterStore,\n",
    "            inputs: NDList,\n",
    "            training: Boolean,\n",
    "            params: PairList<String, Any>?\n",
    "        ): NDList {\n",
    "            val input0 = inputs[0]\n",
    "            val encOutputs = inputs[1]\n",
    "            val envValidLens = inputs[2]\n",
    "//        # During training, all the tokens of any output sequence are processed\n",
    "//        # at the same time, so state[2][self.i] is None as initialized. When\n",
    "//        # decoding any output sequence token by token during prediction,\n",
    "//        # state[2][self.i] contains representations of the decoded output at\n",
    "//        # the i-th block up to the current time step\n",
    "\n",
    "            // TODO FIX IT\n",
    "//            if state[2][self.i] is None:\n",
    "//            key_values = X\n",
    "//            else:\n",
    "//            key_values = torch.cat((state[2][self.i], X), dim=1)\n",
    "//            state[2][self.i] = key_values\n",
    "\n",
    "            val cache = if (inputs.size > 3) inputs[3] else null\n",
    "            val keyValues = if (cache == null) {\n",
    "                input0\n",
    "            } else {\n",
    "                cache.concat(input0, 1)\n",
    "            }\n",
    "            /*\n",
    "            } else if (inputs[3]!!.size(0) < i.toLong()) {\n",
    "                keyValues = inputs[3].concat(inputs[0])\n",
    "//                keyValues = inputs[3]\n",
    "            } else {\n",
    "//                println(inputs[3].get(i.toLong()).concat(input0))\n",
    "//                val keyValue = inputs[3].get(i.toLong()).concat(input0, 1)\n",
    "                keyValues = inputs[3].get(i.toLong()).concat(input0)\n",
    "//                keyValues!!.set(NDIndex(i.toLong()), keyValue)\n",
    "//                if (training) {\n",
    "//                    keyValues = keyValue.expandDims(0)\n",
    "//                } else {\n",
    "//                    keyValues = inputs[3].concat(input0.expandDims(0))\n",
    "//                }\n",
    "            }\n",
    "            println(\"KEYVALUES:: $keyValues\")\n",
    "\n",
    "             */\n",
    "\n",
    "            var decValidLens: NDArray?\n",
    "            if (training) {\n",
    "                val batchSize = input0.shape[0]\n",
    "                val numSteps = input0.shape[1]\n",
    "                //  Shape of dec_valid_lens: (batch_size, num_steps), where every\n",
    "                //  row is [1, 2, ..., num_steps]\n",
    "                decValidLens = manager.arange(1f, (numSteps + 1).toFloat()).reshape(1, numSteps).repeat(0, batchSize)\n",
    "            } else {\n",
    "                decValidLens = null\n",
    "            }\n",
    "//        # Self-attention\n",
    "            val X2 = attention1.forward(ps, NDList(input0, keyValues, keyValues, decValidLens), training)\n",
    "            val Y = addnorm1.forward(ps, NDList(input0, X2.head()), training)\n",
    "//        # Encoder-decoder attention. Shape of enc_outputs:\n",
    "//        # (batch_size, num_steps, num_hiddens)\n",
    "            val Y2 = attention2.forward(ps, NDList(Y.head(), encOutputs, encOutputs, envValidLens), training)\n",
    "            val Z = addnorm2.forward(ps, NDList(Y.head(), Y2.head()), training)\n",
    "            return NDList(addnorm3.forward(ps, NDList(Z.head(), ffn.forward(ps, NDList(Z), training).head()), training).head(), encOutputs, envValidLens, keyValues)\n",
    "        }\n",
    "\n",
    "        override fun getOutputShapes(inputShapes: Array<Shape>): Array<Shape> {\n",
    "            return arrayOf<Shape>()\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf6db4e",
   "metadata": {},
   "source": [
    "To facilitate scaled dot-product operations\n",
    "in the encoder-decoder attention\n",
    "and addition operations in the residual connections,\n",
    "[**the feature dimension (`num_hiddens`) of the decoder is\n",
    "the same as that of the encoder.**]\n",
    "\n",
    "```{.python .input}\n",
    "%%tab mxnet\n",
    "decoder_blk = TransformerDecoderBlock(24, 48, 8, 0.5, 0)\n",
    "decoder_blk.initialize()\n",
    "X = np.ones((2, 100, 24))\n",
    "state = [encoder_blk(X, valid_lens), valid_lens, [None]]\n",
    "d2l.check_shape(decoder_blk(X, state)[0], X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    val decoderBlk = TransformerDecoderBlock(24, 48, 8, 0.5f, 0)\n",
    "    val X = manager.ones(Shape(2, 100, 24))\n",
    "    // Self-contained setup for this demo cell\n",
    "    val validLens = manager.create(floatArrayOf(3f, 2f))\n",
    "    val encoderBlock = TransformerEncoderBlock(24, 48, 8, 0.5f)\n",
    "    encoderBlock.initialize(manager, DataType.FLOAT32, X.shape, validLens.shape)\n",
    "    val input = NDList(X, validLens)\n",
    "\n",
    "    decoderBlk.initialize(manager, DataType.FLOAT32, *input.shapes)\n",
    "    val state = encoderBlock.forward(ps, NDList(X, validLens, validLens), false)\n",
    "    println(decoderBlk.forward(ps, NDList(X, state.head(), validLens, null), false))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473f623d",
   "metadata": {},
   "source": [
    "Now we [**construct the entire Transformer decoder**]\n",
    "composed of `num_blks` instances of `TransformerDecoderBlock`.\n",
    "In the end,\n",
    "a fully connected layer computes the prediction\n",
    "for all the `vocab_size` possible output tokens.\n",
    "Both of the decoder self-attention weights\n",
    "and the encoder-decoder attention weights\n",
    "are stored for later visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a98490",
   "metadata": {},
   "outputs": [],
   "source": [
    "    class TransformerDecoder(\n",
    "        vocabSize: Int,\n",
    "        val numHiddens: Int,\n",
    "        ffnNumHiddens: Int,\n",
    "        numHeads: Int,\n",
    "        val numBlks: Int,\n",
    "        dropout: Float\n",
    "    ) : AttentionDecoder() {\n",
    "        val list: List<String> = (0 until vocabSize).map { it.toString() }\n",
    "        val vocab: Vocabulary = DefaultVocabulary(list)\n",
    "        val embedding = TrainableWordEmbedding.builder()\n",
    "            .optNumEmbeddings(vocabSize)\n",
    "            .setEmbeddingSize(numHiddens)\n",
    "            .setVocabulary(vocab)\n",
    "            .build()\n",
    "        val posEncoding = PositionalEncoding(numHiddens, dropout, 1000, manager)\n",
    "        val blks = mutableListOf<TransformerDecoderBlock>()\n",
    "\n",
    "        //            val attentionWeights = Array<NDArray?>(numBlks) { null }\n",
    "        val linear = Linear.builder().setUnits(vocabSize.toLong()).build()\n",
    "        var attentionWeightsArr2: MutableList<NDArray?>? = null\n",
    "        var attentionWeightsArr1: MutableList<NDArray?>? = null\n",
    "\n",
    "        init {\n",
    "            addChildBlock(\"embedding\", embedding)\n",
    "            repeat(numBlks) {\n",
    "                blks.add(\n",
    "                    TransformerDecoderBlock(\n",
    "                        numHiddens,\n",
    "                        ffnNumHiddens.toLong(),\n",
    "                        numHeads.toLong(),\n",
    "                        dropout,\n",
    "                        it\n",
    "                    )\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "\n",
    "        override fun initState(input: NDList): NDList {\n",
    "            val encOutputs = input[0]\n",
    "            val encValidLens = input[1]\n",
    "            val state = NDList(encOutputs, encValidLens)\n",
    "            repeat(numBlks) {\n",
    "                state.add(null)\n",
    "            }\n",
    "            return state\n",
    "        }\n",
    "\n",
    "        override fun forwardInternal(\n",
    "            ps: ParameterStore,\n",
    "            inputs: NDList,\n",
    "            training: Boolean,\n",
    "            params: PairList<String, Any>?\n",
    "        ): NDList {\n",
    "            var X = inputs[0]\n",
    "            X = X.toType(DataType.INT64, false)\n",
    "            val state = inputs.subNDList(1)\n",
    "            val encOutputs = state[0]\n",
    "            val encValidLens = state[1]\n",
    "            val pos = posEncoding.forward(\n",
    "                ps,\n",
    "                NDList(embedding.forward(ps, NDList(X), training, params).head().mul(Math.sqrt(numHiddens.toDouble()))),\n",
    "                training,\n",
    "                params\n",
    "            )\n",
    "            X = pos.head()\n",
    "            attentionWeightsArr1 = if (!training) mutableListOf() else null\n",
    "            attentionWeightsArr2 = if (!training) mutableListOf() else null\n",
    "            val cacheStart = 2\n",
    "            val newState = NDList(encOutputs, encValidLens)\n",
    "            for (i in 0 until blks.size) {\n",
    "                val cache = if (state.size > cacheStart + i) state[cacheStart + i] else null\n",
    "                val blkOut = blks[i].forward(ps, NDList(X, encOutputs, encValidLens, cache), training, params)\n",
    "                X = blkOut.head()\n",
    "                val newCache = if (blkOut.size > 3) blkOut[3] else null\n",
    "                newState.add(newCache)\n",
    "                if (!training) {\n",
    "                    attentionWeightsArr1!!.add(blks[i].attention1.attention.attentionWeights)\n",
    "                    attentionWeightsArr2!!.add(blks[i].attention2.attention.attentionWeights)\n",
    "                }\n",
    "            }\n",
    "            val ret = linear.forward(ps, NDList(X), training, params)\n",
    "            return NDList(ret.head()).addAll(newState)\n",
    "        }\n",
    "\n",
    "        override fun initializeChildBlocks(manager: NDManager, dataType: DataType, vararg inputShapes: Shape) {\n",
    "            embedding.initialize(manager, dataType, inputShapes[0])\n",
    "            val decShape = inputShapes[0].add(numHiddens.toLong())\n",
    "            posEncoding.initialize(manager, dataType, decShape)\n",
    "            val encOutputsShape = inputShapes[1]\n",
    "            val encValidLensShape = if (inputShapes.size > 2) {\n",
    "                inputShapes[2]\n",
    "            } else {\n",
    "                Shape(encOutputsShape.get(0))\n",
    "            }\n",
    "            for (blk in blks) {\n",
    "                blk.initialize(manager, dataType, decShape, encOutputsShape, encValidLensShape)\n",
    "            }\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891fd389",
   "metadata": {},
   "source": [
    "## [**Training**]\n",
    "\n",
    "Let's instantiate an encoder-decoder model\n",
    "by following the Transformer architecture.\n",
    "Here we specify that\n",
    "both the Transformer encoder and the Transformer decoder\n",
    "have 2 layers using 4-head attention.\n",
    "Similar to :numref:`sec_seq2seq_training`,\n",
    "we train the Transformer model\n",
    "for sequence to sequence learning on the English-French machine translation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    var trainedNet: EncoderDecoder? = null\n",
    "    var trainedSrcVocab: Vocab? = null\n",
    "    var trainedTgtVocab: Vocab? = null\n",
    "    var trainedNumSteps: Int = 0\n",
    "\n",
    "    fun train() {\n",
    "//        num_hiddens, num_blks, dropout = 256, 2, 0.2\n",
    "//        ffn_num_hiddens, num_heads = 64, 4\n",
    "\n",
    "        val numHiddens = 256\n",
    "        val numBlks = 2\n",
    "        val ffnNumHiddens = 64\n",
    "        val numHeads = 4\n",
    "        val batchSize = 2\n",
    "        // Note: 5 epochs is a minimal demo; increase for better quality.\n",
    "        val numEpochs = 5\n",
    "        val numSteps = 35\n",
    "\n",
    "        val dropout = 0.2f\n",
    "        val lr = 0.001f\n",
    "        val device = manager.device\n",
    "\n",
    "        val dataNMT = NMT.loadDataNMT(batchSize, numSteps, 600)\n",
    "        val dataset: ArrayDataset = dataNMT.first\n",
    "        val srcVocab: Vocab = dataNMT.second.first\n",
    "        val tgtVocab: Vocab = dataNMT.second.second\n",
    "\n",
    "        val encoder = TransformerEncoder(srcVocab.length(), numHiddens, ffnNumHiddens.toLong(), numHeads.toLong(), numBlks, dropout)\n",
    "        encoder.initialize(manager, DataType.FLOAT32, Shape(batchSize.toLong(), numSteps.toLong()), Shape(batchSize.toLong()))\n",
    "\n",
    "        val decoder = TransformerDecoder(tgtVocab.length(), numHiddens, ffnNumHiddens, numHeads, numBlks, dropout)\n",
    "        decoder.initialize(manager, DataType.FLOAT32, Shape(batchSize.toLong(), numSteps.toLong()), Shape(batchSize.toLong()))\n",
    "\n",
    "        val net = EncoderDecoder(encoder, decoder)\n",
    "        trainedNet = net\n",
    "        trainedSrcVocab = srcVocab\n",
    "        trainedTgtVocab = tgtVocab\n",
    "        trainedNumSteps = numSteps\n",
    "        fun trainSeq2Seq(\n",
    "            net: EncoderDecoder,\n",
    "            dataset: ArrayDataset,\n",
    "            lr: Float,\n",
    "            numEpochs: Int,\n",
    "            tgtVocab: Vocab,\n",
    "            device: Device\n",
    "        ) {\n",
    "            val loss: Loss = MaskedSoftmaxCELoss()\n",
    "            val lrt: Tracker = Tracker.fixed(lr)\n",
    "            val adam: Optimizer = Optimizer.adam().optLearningRateTracker(lrt).build()\n",
    "            val config: DefaultTrainingConfig = DefaultTrainingConfig(loss)\n",
    "                .optOptimizer(adam) // Optimizer (loss function)\n",
    "                .optInitializer(XavierInitializer(), \"\")\n",
    "            val model: ai.djl.Model = ai.djl.Model.newInstance(\"\")\n",
    "            model.block = net\n",
    "            val trainer: Trainer = model.newTrainer(config)\n",
    "//    val animator = Animator()\n",
    "            var watch: StopWatch\n",
    "            var metric: Accumulator\n",
    "            var lossValue = 0.0\n",
    "            var speed = 0.0\n",
    "            for (epoch in 1..numEpochs) {\n",
    "                watch = StopWatch()\n",
    "                metric = Accumulator(2) // Sum of training loss, no. of tokens\n",
    "                // Iterate over dataset\n",
    "                for (batch in dataset.getData(manager)) {\n",
    "                    val X: NDArray = batch.data.get(0)\n",
    "                    val lenX: NDArray = batch.data.get(1)\n",
    "                    val Y: NDArray = batch.labels.get(0)\n",
    "                    val lenY: NDArray = batch.labels.get(1)\n",
    "                    val bos: NDArray = manager\n",
    "                        .full(Shape(Y.shape[0]), tgtVocab.getIdx(\"<bos>\"))\n",
    "                        .reshape(-1, 1)\n",
    "                    val decInput: NDArray = NDArrays.concat(\n",
    "                        NDList(bos, Y.get(NDIndex(\":, :-1\"))),\n",
    "                        1\n",
    "                    ) // Teacher forcing\n",
    "                    Engine.getInstance().newGradientCollector().use { gc ->\n",
    "                        val yHat: NDArray = net.forward(\n",
    "                            ParameterStore(manager, false),\n",
    "                            NDList(X, decInput, lenX),\n",
    "                            true\n",
    "                        )\n",
    "                            .get(0)\n",
    "                        val l = loss.evaluate(NDList(Y, lenY), NDList(yHat))\n",
    "                        gc.backward(l)\n",
    "                        metric.add(floatArrayOf(l.sum().getFloat(), lenY.sum().getLong().toFloat()))\n",
    "                    }\n",
    "                    TrainingChapter9.gradClipping(net, 1, manager)\n",
    "                    // Update parameters\n",
    "                    trainer.step()\n",
    "                }\n",
    "                lossValue = metric.get(0).toDouble() / metric.get(1)\n",
    "                speed = metric.get(1) / watch.stop()\n",
    "//                if ((epoch + 1) % 10 == 0) {\n",
    "//            animator.add(epoch + 1, lossValue.toFloat(), \"loss\")\n",
    "//            animator.show()\n",
    "                println(\"${epoch + 1} : $lossValue\")\n",
    "//                }\n",
    "            }\n",
    "            println(\"loss: %.3f, %.1f tokens/sec on %s%n\".format(lossValue, speed, device.toString()))\n",
    "        }\n",
    "        trainSeq2Seq(net, dataset, lr, numEpochs, tgtVocab, device)\n",
    "    }\n",
    "    this.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af5db0",
   "metadata": {},
   "source": [
    "After training,\n",
    "we use the Transformer model\n",
    "to [**translate a few English sentences**] into French and compute their BLEU scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14971fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "        fun predictSeq2Seq(\n",
    "            net: EncoderDecoder,\n",
    "            srcSentence: String,\n",
    "            srcVocab: Vocab,\n",
    "            tgtVocab: Vocab,\n",
    "            numSteps: Int,\n",
    "            saveAttentionWeights: Boolean\n",
    "        ): Pair<String, List<NDArray?>> {\n",
    "            val srcTokens = srcVocab.getIdxs(srcSentence.lowercase(Locale.getDefault()).split(\" \")) + listOf(srcVocab.getIdx(\"<eos>\"))\n",
    "            val encValidLen = manager.create(srcTokens.size).reshape(1)\n",
    "            val truncateSrcTokens = NMT.truncatePad(srcTokens, numSteps, srcVocab.getIdx(\"<pad>\"))\n",
    "            // Add the batch axis\n",
    "            val encX = manager.create(truncateSrcTokens.toIntArray()).expandDims(0)\n",
    "            val encOutputs = net.encoder.forward(ParameterStore(manager, false), NDList(encX, encValidLen), false)\n",
    "            var decState = net.decoder.initState(encOutputs)\n",
    "            // Add the batch axis\n",
    "            var decX = manager.create(floatArrayOf(tgtVocab.getIdx(\"<bos>\").toFloat())).repeat(35).expandDims(0)\n",
    "            val outputSeq: MutableList<Int> = mutableListOf()\n",
    "            val attentionWeightSeq: MutableList<NDArray?> = mutableListOf()\n",
    "            for (i in 0 until numSteps) {\n",
    "//                println(i)\n",
    "                val output = net.decoder.forward(\n",
    "                    ParameterStore(manager, false),\n",
    "                    NDList(decX).addAll(decState),\n",
    "                    false\n",
    "                )\n",
    "\n",
    "//                val encOutputs = encoder.forward(parameterStore, encX, training, params)\n",
    "//                val decState = decoder.initState(encOutputs)\n",
    "//                val inp = NDList(decX).addAll(decState)\n",
    "//                return decoder.forward(parameterStore, inp, training, params)\n",
    "\n",
    "                val Y = output[0]\n",
    "                decState = output.subNDList(1)\n",
    "                // We use the token with the highest prediction likelihood as the input\n",
    "                // of the decoder at the next time step\n",
    "//                println(\"Y:::$Y\")\n",
    "//                println(\"Y(1)::: ${Y.get(NDIndex(\"0,2\")).argMax(0).getLong().toInt()}\")\n",
    "//                decX = Y.argMax(2)\n",
    "//                println(\"DECX: ${decX.squeeze(0)}\")\n",
    "//                val pred = decX.squeeze(0).getLong().toInt()\n",
    "                val pred = Y.get(NDIndex(\"0,2\")).argMax(0).getLong().toInt()\n",
    "                // Save attention weights (to be covered later)\n",
    "                if (saveAttentionWeights) {\n",
    "                    attentionWeightSeq.add(net.decoder.attentionWeights)\n",
    "                }\n",
    "                // Once the end-of-sequence token is predicted, the generation of the\n",
    "                // output sequence is complete\n",
    "                if (pred == tgtVocab.getIdx(\"<eos>\")) {\n",
    "                    break\n",
    "                }\n",
    "                outputSeq.add(pred)\n",
    "            }\n",
    "            val outputString: String = tgtVocab.toTokens(outputSeq).joinToString(separator = \" \")\n",
    "            return Pair(outputString, attentionWeightSeq.toList())\n",
    "        }\n",
    "\n",
    "        /* Compute the BLEU. */\n",
    "        fun bleu(predSeq: String, labelSeq: String, k: Int): Double {\n",
    "            val predTokens = predSeq.split(\" \")\n",
    "            val labelTokens = labelSeq.split(\" \")\n",
    "            val lenPred = predTokens.size\n",
    "            val lenLabel = labelTokens.size\n",
    "            var score = Math.exp(Math.min(0.toDouble(), 1.0 - lenLabel / lenPred))\n",
    "            for (n in 1 until k + 1) {\n",
    "                var numMatches = 0\n",
    "                val labelSubs = mutableMapOf<String, Int>()\n",
    "                for (i in 0 until lenLabel - n + 1) {\n",
    "                    val key = labelTokens.subList(i, i + n).joinToString(separator = \" \")\n",
    "                    labelSubs.put(key, labelSubs.getOrDefault(key, 0) + 1)\n",
    "                }\n",
    "                for (i in 0 until lenPred - n + 1) {\n",
    "                    // val key =predTokens.subList(i, i + n).joinToString(\" \")\n",
    "                    val key = predTokens.subList(i, i + n).joinToString(separator = \" \")\n",
    "                    if (labelSubs.getOrDefault(key, 0) > 0) {\n",
    "                        numMatches += 1\n",
    "                        labelSubs.put(key, labelSubs.getOrDefault(key, 0) - 1)\n",
    "                    }\n",
    "                }\n",
    "                score *= Math.pow(numMatches.toDouble() / (lenPred - n + 1).toDouble(), Math.pow(0.5, n.toDouble()))\n",
    "            }\n",
    "            return score\n",
    "        }\n",
    "\n",
    "        val net = trainedNet ?: error(\"Run the training cell first\")\n",
    "        val srcVocab = trainedSrcVocab ?: error(\"Run the training cell first\")\n",
    "        val tgtVocab = trainedTgtVocab ?: error(\"Run the training cell first\")\n",
    "        val numSteps = trainedNumSteps\n",
    "\n",
    "        val engs = arrayOf(\"go .\", \"i lost .\", \"he's calm .\", \"i'm home .\")\n",
    "        val fras = arrayOf(\"va !\", \"j'ai perdu .\", \"il est calme .\", \"je suis chez moi .\")\n",
    "        for (i in engs.indices) {\n",
    "            val pair = predictSeq2Seq(net, engs[i], srcVocab, tgtVocab, numSteps, false)\n",
    "            val translation: String = pair.first\n",
    "            println(\"%s => %s, bleu %.3f\".format(engs[i], translation, bleu(translation, fras[i], 2)))\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a67bff",
   "metadata": {},
   "source": [
    "Let's [**visualize the Transformer attention weights**] when translating the last English sentence into French.\n",
    "The shape of the encoder self-attention weights\n",
    "is (number of encoder layers, number of attention heads, `num_steps` or number of queries, `num_steps` or number of key-value pairs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84266646",
   "metadata": {},
   "source": [
    "In the encoder self-attention,\n",
    "both queries and keys come from the same input sequence.\n",
    "Since padding tokens do not carry meaning,\n",
    "with specified valid length of the input sequence,\n",
    "no query attends to positions of padding tokens.\n",
    "In the following,\n",
    "two layers of multi-head attention weights\n",
    "are presented row by row.\n",
    "Each head independently attends\n",
    "based on a separate representation subspaces of queries, keys, and values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85f09e8",
   "metadata": {},
   "source": [
    "[**To visualize both the decoder self-attention weights and the encoder-decoder attention weights,\n",
    "we need more data manipulations.**]\n",
    "For example,\n",
    "we fill the masked attention weights with zero.\n",
    "Note that\n",
    "the decoder self-attention weights\n",
    "and the encoder-decoder attention weights\n",
    "both have the same queries:\n",
    "the beginning-of-sequence token followed by\n",
    "the output tokens and possibly\n",
    "end-of-sequence tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e539b6",
   "metadata": {},
   "source": [
    "Due to the auto-regressive property of the decoder self-attention,\n",
    "no query attends to key-value pairs after the query position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15fb238",
   "metadata": {},
   "source": [
    "Similar to the case in the encoder self-attention,\n",
    "via the specified valid length of the input sequence,\n",
    "[**no query from the output sequence\n",
    "attends to those padding tokens from the input sequence.**]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb3af8",
   "metadata": {},
   "source": [
    "Although the Transformer architecture\n",
    "was originally proposed for sequence-to-sequence learning,\n",
    "as we will discover later in the book,\n",
    "either the Transformer encoder\n",
    "or the Transformer decoder\n",
    "is often individually used\n",
    "for different deep learning tasks.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "The Transformer is an instance of the encoder-decoder architecture, \n",
    "though either the encoder or the decoder can be used individually in practice.\n",
    "In the Transformer architecture, multi-head self-attention is used \n",
    "for representing the input sequence and the output sequence, \n",
    "though the decoder has to preserve the auto-regressive property via a masked version.\n",
    "Both the residual connections and the layer normalization in the Transformer\n",
    "are important for training a very deep model.\n",
    "The positionwise feed-forward network in the Transformer model \n",
    "transforms the representation at all the sequence positions using the same MLP.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Train a deeper Transformer in the experiments. How does it affect the training speed and the translation performance?\n",
    "1. Is it a good idea to replace scaled dot-product attention with additive attention in the Transformer? Why?\n",
    "1. For language modeling, should we use the Transformer encoder, decoder, or both? How to design this method?\n",
    "1. What can be challenges to Transformers if input sequences are very long? Why?\n",
    "1. How to improve computational and memory efficiency of Transformers? Hint: you may refer to the survey paper by :citet:`Tay.Dehghani.Bahri.ea.2020`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52cc9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "2.4.0-dev-539"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
